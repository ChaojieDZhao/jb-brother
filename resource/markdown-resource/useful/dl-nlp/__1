1、深度学习这个术语来指训练神经网络的过程。有时它指的是特别大规模的神经网络训练。
2、线性回归是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法，运用十分广泛。其表达形式为y = w'x+e，e为误差服从均值为0的正态分布。 [1]
回归分析中，只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，这种回归分析称为一元线性回归分析。如果回归分析中包括两个或两个以上的自变量，且因变量和自变量之间是线性关系，则称为多元线性回归分析。
3、正态分布（Normal distribution），也称“常态分布”，又名高斯分布（Gaussian distribution）。正态曲线呈钟型，两头低，中间高，左右对称因其曲线呈钟形，因此人们又经常称之为钟形曲线。
4、ReLU （修正线性单元）激活函数，它的全称是Rectified Linear Unit。从趋近于零开始，然后变成一条直线。
5、给予了足够的训练样本有关𝑥 和𝑦。神经网络非常擅长计算从𝑥到𝑦的精准映射函数。
6、对于图像应用，我们经常在神经网络上使用卷积（Convolutional Neural Network）。对于序列数据，经常使用 RNN，一种递归神经网络（Recurrent Neural Network）。
7、多亏了深度学习和神经网络，计算机现在能更好地解释非结构化数据。
8、如今最可靠的方法来在神经网络上获得更好的性能，往往就是要么训练一个更大的神经网络，要么投入更多的数据，一个带有许多隐藏单元的神经网络，也有许多的参数及关联性，就如同需要大规模的数据一样。同样可以带来更好的性能。
9、Sigmoid激活函数是一个在生物学中常见的S型函数，也称为S型生长曲线。Sigmoid函数常被用作神经网络的激活函数，将变量映射到（0,1）之间。（图像解释见note）
10、激活函数（Activation Function），就是在人工神经网络的神经元上运行的函数，负责将神经元的输入映射到输出端。
*11、
Mtrain代表训练集，Mtest代表测试集。
𝑋： [𝑥(1), 𝑥(2), . . . , 𝑥(𝑚)]：表示所有的训练数据集的输入值，放在一个 𝑛𝑥 × 𝑚的矩阵中，
其中𝑚表示样本数目。
𝑌 = [𝑦(1), 𝑦(2), . . . , 𝑦(𝑚)]：对应表示所有训练数据集的输出值，维度为1 × 𝑚。
x：表示一个nx维数据，为输入数据，维度为(nx,1)，就是一个列向量。
y：表示输出结果，取值为（0，1）。
𝑧：来表示𝑤𝑇（转置）𝑥 + 𝑏的值。
𝑤：来表示逻辑回归的参数，这也是一个𝑛𝑥维向量（因为𝑤实际上是特征权重，维度与特征向量相同）。
𝑥(𝑖)：来表示第𝑖个训练样本。
𝑥(𝑖)<𝑡>：训练样本𝑖的序列中第𝑡个元素。
𝑇𝑥(𝑖)：第𝑖个训练样本的输入序列长度。
𝑦(𝑖)<𝑡>：代表第𝑖个训练样本输出的第𝑡个元W素。
𝑇𝑦(𝑖)就是第𝑖个训练样本的输出序列的长度。
12、为了训练逻辑回归模型的参数𝑤和参数𝑏。我们需要一个代价函数，通过训练代价函数来得到参数𝑤和参数𝑏。
13、逻辑回归的损失函数是：𝐿(𝑦^ , 𝑦) = −𝑦log(𝑦^) − (1 − 𝑦)log(1 − 𝑦^)。（定理，只管使用就行）
14、损失函数既是在单个训练样本中定义的，它衡量的是算法在单个训练样本中表现如何，为了衡量算法在全部训练样本上的表现如何，我们需要定义一个算法的代价函数，即𝐽(𝑤, 𝑏)，算法的代价函数是对𝑚个样本的损失函数求和然后除以𝑚。（图像解释见note）
15、凸函数(convex function)，像一个大碗一样。我们必须定义代价函数（成本函数）𝐽(𝑤, 𝑏)为凸函数。
16、小写字母𝑑 用在求导数（derivative），即函数只有一个参数， 偏导数符号𝜕 用在求偏导（partial derivative），即函数含有两个以上的参数。
17、超参数，比如算法中的 learning rate 𝑎（学习率）、iterations(梯度下降法循环的数量)、𝐿（隐藏层数目）、𝑛[𝑙]（隐藏层单元数目）、choice of activation function（激活函数的选择）都需要你来设置，这些数字实际上控制了最后的参数𝑊和𝑏的值，所以它们被称作超参数。
18、线性linear，指量与量之间按比例、成直线的关系，在数学上可以理解为一阶导数为常数的函数；非线性non-linear则指不按比例、不成直线的关系，一阶导数不为常数。
19、导数，导数就是斜率，而函数的斜率，在不同的点是不同的。在第一个例子中𝑓(𝑎) = 3𝑎 ，这是一条直线，在任何点它的斜率都是相同的，均为 3。但是对于函数𝑓(𝑎) = a2 ，或者𝑓(𝑎) = log𝑎，它们的斜率是变化的，所以它们的导数或者斜率，在曲线上不同的点处是不同的。如果你想知道一个函数的导数，你可参考你的微积分课本或者维基百科，然后你应该就能找到这些函数的导数公式。
20、机器学习和深度学习的区别（还有人工智能）。（图像解释见note）
21、Cross Entropy Loss，交叉熵。分类问题常用的损失函数为交叉熵( Cross Entropy Loss)。交叉熵描述了两个概率分布之间的距离，当交叉熵越小说明二者之间越接近。
22、权重w与偏置b
23、交叉验证与训练集、验证集、测试集。训练集用来训练模型，即确定模型的权重和偏置这些参数，通常我们称这些参数为学习参数。验证集用于模型的选择，更具体地来说，验证集并不参与学习参数的确定，也就是验证集并没有参与梯度下降的过程。验证集只是为了选择超参数，比如网络层数、网络节点数、迭代次数、学习率这些都叫超参数。比如在k-NN算法中，k值就是一个超参数。所以可以使用验证集来求出误差率最小的k。测试集只使用一次，即在训练完成后评价最终的模型时使用。它既不参与学习参数过程，也不参数超参数选择过程，而仅仅使用于模型的评价。值得注意的是，千万不能在训练过程中使用测试集，而后再用相同的测试集去测试模型。这样做其实是一个cheat，使得模型测试时准确率很高。
24、交叉验证训练集、主要是对训练集，假设将训练集分成5份（该数目被称为折数，5-fold交叉验证），每次都用其中4份来训练模型，粉红色的那份用来验证4份训练出来的模型的准确率，记下准确率。然后在这5份中取另外4份做训练集，1份做验证集，再次得到一个模型的准确率。直到所有5份都做过1次验证集，也即验证集名额循环了一圈，交叉验证的过程就结束。算得这5次准确率的均值。留下准确率最高的模型，即该模型的超参数是什么样的最终模型的超参数就是这个样的。
25、ckpt文件是二进制文件，保存了所有的weights、biases、gradients等变量。

、one-hot 向量，因为它只有一个值是 1，其余值都是 0，所以你会有 9 个 one-hot 向量来表示这个句中的 9 个单词。行维度就是单词数量。
、循环神经网络，在每一个时间步中，循环神经网络传递一个激活值到下一个时间步中用于计算。并且会有一个初始的激活值，激活值𝑎<0>。通常使用零向量。（图像解释见note）
、循环神经网络，每个时间步使用的都是相同的参数𝑊ax。激活值也就是水平联系是由参数𝑊𝑎𝑎决定的，同时每一个时间步都使用相同的参数𝑊𝑎𝑎，同样的输出结果由𝑊ya决定。
、循环神经网络的前向传播，一个缺点就是它只使用了这个序列中之前的信息来做出预测，尤其当预测𝑦^<3>时，它没有用到𝑥<4>，𝑥<5>，𝑥<6>等等的信息。（图像解释见note）
、[𝑊𝑎𝑎 ⋮ 𝑊𝑎𝑥] = 𝑊𝑎，这个代表的是矩阵水平并列放置。[𝑎<𝑡−1>, 𝑥<𝑡>]的意思是将这两个向量上下堆在一起。
、循环神经网络的类型。one-to-one。one-to-many,如音乐生成。any-to-many，如机器翻译（encoder-decoder）。many-to-one，如情感分类。（图像解释见note）
、语料库是自然语言处理的一个专有名词，意思就是很长的或者说数量众多的英文句子组成的文本。
、标识化的过程，就是输入的句子都映射到了各个标志上，或者说字典中的各个词上。比如将每个单词都转换成对应的 one-hot 向量。
、Softmax激活函数，它把一些输出的神经元映射到（0-1）之间的实数，并且归一化保证和为1，从而使得多分类的概率之和也刚好为1。
、序列模型模拟了任意特定单词序列的概率，我们要做的就是对这些概率分布进行采样来生成一个新的单词序列。下。
、GRU，Gated Recurrent Unit，是门控循环单元。它改变了 RNN 的隐藏层，使其可以更好地捕捉深层连接，并改善了梯度消失问题。
、LSTMu 长期记忆网络模型单元，GRU和它都是神经网络的一个节点。
、Vanishing gradients with RNNs，循环神经网络的梯度消失。（图像解释见note）
、BRNN双向循环神经网络，先进行前向传播，后进行反向传播，反向传播的输出值后影响前向传播计算的输出值（y^）。缺点就是双向 RNN 模型需要你考虑整个语音表达，但是如果直接用这个去实现的话，你需要等待这个人说完，然后获取整个语音表达才能处理这段语音，并进一步做语音识别。
、词嵌入（word embeddings），语言表示的一种方式，可以让算法自动的理解一些类似的词，比如男人对女人，比如国王对王后，还有其他很多的例子。通过词嵌入的概念你就可以构建 NLP 应用了，即使你的模型标记的训练集相对较小。
、泛化，在机器学习方法中，泛化能力通俗来讲就是指学习到的模型对未知数据的预测能力。在实际情况中，我们通常通过测试误差来评价学习方法的泛化能力。如果在不考虑数据量不足的情况下出现模型的泛化能力差，那么其原因基本为对损失函数的优化没有达到全局最优。
、t-SNE 算法，一种可视化算法，把这个空间映射到低维空间，你可以画出一个 2 维图像然后观察，这就是这个术语嵌入的来源。
、词嵌入，word embedding。在命名实体识别，用在文本摘要，用在文本解析、指代消解。而词嵌入在语言模型、机器翻译领域用的少一些，因为本身语言模型和机器翻译就需要大量的数据。（图像解释见note）
、词嵌入的无监督学习和迁移学习。
、词嵌入迁移学习的步骤。（图像解释见note）
、词嵌入的固定性质。人脸识别中的算法未来可能涉及到海量的人脸照片，而自然语言处理有一个固定的词汇表，而像一些没有出现过的词我们就记为未知单词。
、余弦相似度函数，可以在词嵌入中获取类比的单词。
、来学习词嵌入（300*10000）时，实际上是学习一个嵌入矩阵。矩阵𝐸和这个one-hot向量相乘，最后得到的其实就是这个300维的列，它等于𝑒6257，这个符号是我们用来表示这个 300×1 的嵌入向量的符号，它表示的单词是orange。也就是说要得到一个单词的嵌入向量，直接使用嵌入矩阵和这个单词的ont-hot表示相乘即可。
、Embedding Matrix（嵌入矩阵），矩阵𝐸它包含了词汇表中所有单词的嵌入向量。
、实践证明，建立一个语言模型是学习词嵌入的好方法。
、语言模型。（图像解释见note）
、负采样（Negative Sampling），将softmax转化为sigmoid分类函数。可以减少计算机的工作量。因为sigmoid这个算法计算成本更低，因为只需更新𝐾 + 1个逻辑单元，𝐾 + 1个二分类问题，相对而言每次迭代的成本比更新 10000 维的 softmax 分类器成本低.
、GloVe 代表用词表示的全局变量（global vectors for word representation）
、减少或者是消除学习算法中的偏见问题是个十分重要的问题，因为这些算法会用来辅助制定越来越多的社会中的重要决策。
、机器翻译模型和语言模型的区别：，机器翻译模型其实和语言模型非常相似，不同在于语言模型总是以零向开始，而 encoder 网络会计算出一系列向量来表示输入的句子。（图像解释见note）
、机器翻译模型你要找到最有可能的英语句子，最可能的英语翻译，但是可能的句子组合数量过于巨大，无法一一列举，所以我们需要一种合适的搜索算法，常用的就是集束搜索（Beam Search）。
、集束搜索（Beam Search）的解释。（图像解释见note）
、代数余子式。（图像解释见note）

